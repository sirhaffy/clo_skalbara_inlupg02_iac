---
name: 'Infrastructure Deploy (Terraform + Ansible)'

on:
  pull_request:
    paths:
      - 'Terraform/**'
      - 'Ansible/**'
  push:
    branches:
      - main
    paths:
      - 'Terraform/**'
      - 'Ansible/**'
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform'
        required: true
        default: 'deploy'
        type: choice
        options:
        - deploy
        - destroy

env:
  TF_VERSION: 1.5.0
  AWS_REGION: eu-north-1

jobs:
  # Setup S3 and DynamoDB for Terraform backend
  setup-backend:
    name: 'Setup Terraform Backend (S3 + DynamoDB)'
    runs-on: ubuntu-latest
    if: (github.ref == 'refs/heads/main' && github.event_name == 'push') || (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'deploy')

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Get configuration from Parameter Store
      run: |
        # Try to get parameters, create with defaults if they don't exist
        STATE_BUCKET_NAME=$(aws ssm get-parameter --name "/terraform/state-bucket-name" --query "Parameter.Value" --output text 2>/dev/null || echo "")
        LOCK_TABLE_NAME=$(aws ssm get-parameter --name "/terraform/lock-table-name" --query "Parameter.Value" --output text 2>/dev/null || echo "")

        # If parameters don't exist, create them with default values
        if [ -z "$STATE_BUCKET_NAME" ]; then
          STATE_BUCKET_NAME="clofresva-skalbara-upg02-terraform-state"
          aws ssm put-parameter --name "/terraform/state-bucket-name" --value "$STATE_BUCKET_NAME" --type "String" --description "Terraform state S3 bucket name"
          echo "Created parameter /terraform/state-bucket-name with value: $STATE_BUCKET_NAME"
        fi

        if [ -z "$LOCK_TABLE_NAME" ]; then
          LOCK_TABLE_NAME="terraform-state-lock"
          aws ssm put-parameter --name "/terraform/lock-table-name" --value "$LOCK_TABLE_NAME" --type "String" --description "Terraform state lock DynamoDB table name"
          echo "Created parameter /terraform/lock-table-name with value: $LOCK_TABLE_NAME"
        fi

        # Export to environment for subsequent steps
        echo "STATE_BUCKET_NAME=$STATE_BUCKET_NAME" >> $GITHUB_ENV
        echo "LOCK_TABLE_NAME=$LOCK_TABLE_NAME" >> $GITHUB_ENV
        echo "Using STATE_BUCKET_NAME: $STATE_BUCKET_NAME"
        echo "Using LOCK_TABLE_NAME: $LOCK_TABLE_NAME"

    - name: Check if S3 bucket exists and create if needed
      run: |
        echo "Checking if S3 bucket exists: $STATE_BUCKET_NAME"
        if aws s3api head-bucket --bucket "$STATE_BUCKET_NAME" 2>/dev/null; then
          echo "âœ… S3 bucket already exists: $STATE_BUCKET_NAME"
        else
          echo "ðŸ”„ Creating S3 bucket: $STATE_BUCKET_NAME"
          aws s3 mb "s3://$STATE_BUCKET_NAME" --region "$AWS_REGION"

          # Enable versioning
          aws s3api put-bucket-versioning \
            --bucket "$STATE_BUCKET_NAME" \
            --versioning-configuration Status=Enabled

          # Enable encryption
          aws s3api put-bucket-encryption \
            --bucket "$STATE_BUCKET_NAME" \
            --server-side-encryption-configuration '{
              "Rules": [{
                "ApplyServerSideEncryptionByDefault": {
                  "SSEAlgorithm": "AES256"
                }
              }]
            }'

          # Block public access
          aws s3api put-public-access-block \
            --bucket "$STATE_BUCKET_NAME" \
            --public-access-block-configuration \
            BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true

          echo "âœ… S3 bucket created and configured: $STATE_BUCKET_NAME"
        fi

    - name: Check if DynamoDB table exists and create if needed
      run: |
        echo "Checking if DynamoDB table exists: $LOCK_TABLE_NAME"
        if aws dynamodb describe-table --table-name "$LOCK_TABLE_NAME" --region "$AWS_REGION" 2>/dev/null; then
          echo "âœ… DynamoDB table already exists: $LOCK_TABLE_NAME"
        else
          echo "ðŸ”„ Creating DynamoDB table: $LOCK_TABLE_NAME"
          aws dynamodb create-table \
            --table-name "$LOCK_TABLE_NAME" \
            --attribute-definitions AttributeName=LockID,AttributeType=S \
            --key-schema AttributeName=LockID,KeyType=HASH \
            --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 \
            --region "$AWS_REGION"

          echo "â³ Waiting for table to be created..."
          aws dynamodb wait table-exists --table-name "$LOCK_TABLE_NAME" --region "$AWS_REGION"
          echo "âœ… DynamoDB table created: $LOCK_TABLE_NAME"
        fi

  # Terraform validation (on PRs)
  terraform-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Terraform Init
      working-directory: ./Terraform
      run: terraform init

    - name: Terraform Format Check
      working-directory: ./Terraform
      run: terraform fmt -check

    - name: Terraform Validate
      working-directory: ./Terraform
      run: terraform validate

    - name: Terraform Plan
      working-directory: ./Terraform
      run: terraform plan -no-color
      env:
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_GH_ACTIONS_USER_NAME: ${{ secrets.GH_ACTIONS_USER_NAME }}

  # Ansible validation (on PRs)
  ansible-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Ansible
      run: |
        sudo apt-get update
        sudo apt-get install -y ansible

    - name: Ansible Syntax Check
      working-directory: ./Ansible
      run: |
        ansible-playbook --syntax-check playbooks/docker-swarm.yml

  # Terraform deployment (main branch only)
  terraform-apply:
    runs-on: ubuntu-latest
    if: (github.ref == 'refs/heads/main' && github.event_name == 'push') || (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'deploy')
    needs: setup-backend

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Terraform Init
      working-directory: ./Terraform
      run: terraform init

    - name: Terraform Plan
      working-directory: ./Terraform
      run: terraform plan -out=tfplan
      env:
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_GH_ACTIONS_USER_NAME: ${{ secrets.GH_ACTIONS_USER_NAME }}

    - name: Terraform Apply
      working-directory: ./Terraform
      run: terraform apply -auto-approve tfplan

  # Ansible deployment (after Terraform completes)
  ansible-deploy:
    runs-on: ubuntu-latest
    if: (github.ref == 'refs/heads/main' && github.event_name == 'push') || (github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'deploy')
    needs: terraform-apply  # WAIT for Terraform to finish first

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Install Ansible
      run: |
        sudo apt-get update
        sudo apt-get install -y ansible

    - name: Setup SSH key
      run: |
        mkdir -p ~/.ssh
        echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/clo_ec2_001
        chmod 600 ~/.ssh/clo_ec2_001

    - name: Generate dynamic Ansible inventory
      working-directory: ./Terraform
      run: |
        # Get EC2 instance IPs from Terraform output
        MANAGER_IPS=$(terraform output -json swarm_manager_ips | jq -r '.[]')
        WORKER_IPS=$(terraform output -json swarm_worker_ips | jq -r '.[]')

        # Create dynamic inventory
        cat > ../Ansible/inventory/dynamic_hosts.yml << EOF
        ---
        all:
          vars:
            ansible_user: ec2-user
            ansible_ssh_private_key_file: ~/.ssh/clo_ec2_001
            ansible_ssh_common_args: '-o StrictHostKeyChecking=no'

        docker_swarm:
          children:
            swarm_managers:
              hosts:
        EOF

        # Add manager hosts
        counter=1
        for ip in $MANAGER_IPS; do
          cat >> ../Ansible/inventory/dynamic_hosts.yml << EOF
                swarm-manager-0$counter:
                  ansible_host: $ip
                  swarm_role: manager
                  swarm_primary: $([ $counter -eq 1 ] && echo "true" || echo "false")
        EOF
          counter=$((counter + 1))
        done

        # Add workers section
        cat >> ../Ansible/inventory/dynamic_hosts.yml << EOF

            swarm_workers:
              hosts:
        EOF

        # Add worker hosts
        counter=1
        for ip in $WORKER_IPS; do
          cat >> ../Ansible/inventory/dynamic_hosts.yml << EOF
                swarm-worker-0$counter:
                  ansible_host: $ip
                  swarm_role: worker
        EOF
          counter=$((counter + 1))
        done

        echo "Generated dynamic inventory:"
        cat ../Ansible/inventory/dynamic_hosts.yml

    - name: Run Ansible Playbook
      working-directory: ./Ansible
      run: |
        ansible-playbook -i inventory/dynamic_hosts.yml \
          playbooks/docker-swarm.yml

  # Destroy infrastructure (manual trigger only)
  destroy:
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.action == 'destroy'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ${{ env.TF_VERSION }}

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Terraform Init
      working-directory: ./Terraform
      run: terraform init

    - name: Terraform Destroy
      working-directory: ./Terraform
      run: terraform destroy -auto-approve
      env:
        TF_VAR_ssh_key_name: ${{ secrets.SSH_KEY_NAME }}
        TF_VAR_GH_ACTIONS_USER_NAME: ${{ secrets.GH_ACTIONS_USER_NAME }}

    - name: Cleanup S3 bucket and DynamoDB (optional)
      run: |
        echo "Removing Terraform state resources..."
        aws s3 rm "s3://$STATE_BUCKET_NAME" --recursive || true
        aws s3 rb "s3://$STATE_BUCKET_NAME" || true
        aws dynamodb delete-table --table-name "$LOCK_TABLE_NAME" --region "$AWS_REGION" || true
        echo "Cleanup completed"